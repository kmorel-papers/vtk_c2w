% -*- latex -*-

\documentclass[twocolumn]{article}

\usepackage[usenames]{color}
\definecolor{Red}{rgb}{1,0,0}
\newcommand{\sticky}[1]{\textcolor{Red}{[#1]}}
%\newcommand{\sticky}[1]{}

\usepackage{graphicx}
\usepackage{varioref}
\usepackage{hyperref}

\newcommand{\cidentifier}[1]{\texttt{#1}}

\title{From Cluster to Wall with VTK}
\author{Kenneth~Moreland and David~Thompson}

\begin{document}

  \maketitle

  \begin{abstract}
    This paper describes new parallel rendering component for VTK, the
    Visualization Toolkit.  The parallel rendering unit allows for vast
    quantities of geometry to be rendered on cluster computers.
    Furthermore, the geometry may be displayed on tiled displays at full or
    reduced resolution.  We have demonstrated an interactive VTK
    application processing an isosurface consisting of nearly half a
    billion triangles and displaying on a power wall with a total
    resolution of 60 Mega-pixels ($60*2^{20}$ pixels).  We also demonstrate
    an interactive VTK application displaying the same geometry on a
    desktop connected to the cluster via a TCP/IP socket over 100BASE-T
    Ethernet.
  \end{abstract}

  \section{Introduction}
  \label{sec:introduction}

  ``That's great.  How can I use it?''  This is the question our
  visualization research team is faced with whenever we present a promising
  new tool to our analysts.  Until recently, our best solution was to wrap
  the tool in its own user interface.  This proliferation of visualization
  tools wastes human resources on many fronts.  It requires our researchers
  to design, build, and maintain user interfaces for their tools.  Since
  our visualization researchers are often neither motivated nor experienced
  at designing user interfaces, the tool interfaces are often
  ill-conceived, rarely consistent, and never tightly coupled.  Analysts
  are forced to learn a wide variety of interfaces for the tools they wish
  to use, if they are aware of the tools' existence at all.  Often this
  results in tools being unused.

  To alleviate these problems, we have chosen to leverage VTK, the
  Visualization Toolkit \cite{Schroeder98}.  VTK is an open-source API
  containing a comprehensive suite of visualization tools.  More
  importantly, VTK incorporates an extendible component-based architecture.
  Our new approach to delivering visualization tools is to wrap these tools
  into VTK components and store these components in a common component
  library.  This should allow each tool to work together with current and
  future tools under the same, consistent user interface.  All three DOE
  national labs, Sandia, Las Alamos, and Lawrence Livermore, have agreed to
  make VTK the R\&D platform for visualization.

  Our visualization needs put a large strain on VTK.  ASCI routinely
  creates simulations on the order of 50 million cells \cite{Heermann99},
  and it is predicted that by 2004 ASCI will routinely create 250 million
  cell simulations requiring up to a petabyte of storage \cite{Smith98}.
  The processing of such data is well beyond the capabilities of a standard
  workstation.  Our most cost effective means for handling this data in a
  timely manner is the use of commodity cluster computers running
  distributed memory algorithms.  The creation of such high fidelity models
  also necessitates the use of high fidelity displays for visualization.
  To realize these high resolution displays we build ``power walls,''
  projected displays driven by tiled arrays of commodity projectors.  We
  require our VTK applications to run interactively on clustered computers
  while either driving a power wall or shipping images back to a remote
  computer for desktop delivery.


  \section{Previous Work}
  \label{sec:previous_work}

  Thanks to a recent collaboration between Kitware and the ASCI/VIEWS
  program, VTK now supports parallel programming and can be run on cluster
  computers \cite{Ahrens00}.  The parallel VTK libraries support several
  parallel modes.  The mode we concern ourselves with in this paper is
  ``data'' parallelism in which the input data is split amongst processes,
  the pieces are filtered and rendered independently, and then composited
  to one image in the end.  This method allows us to make effective use of
  the parallel and distributed nature of our clusters, but can break down
  if the model pieces are too big to be processed by the available
  resources\footnote{Ahrens et. al. \cite{Ahrens01} describe using VTK in a
  streaming mode to perform out-of-core processing of data.}.

  While parallel visualization and rendering are now supported by VTK,
  display to a power wall and desktop delivery currently are not.  One
  possible approach is to use Chromium \cite{Humphreys02}.  Chromium is
  capable of replacing the OpenGL library loaded by an application at
  runtime, intercepting the stream of OpenGL commands, and performing
  several altercations to it, including driving a power wall.
  Unfortunately, an OpenGL stream is inherently serial and must be issued
  from a single process, which is a huge bottleneck when dealing with large
  amounts of data.  Chromium also supports a parallel rendering mode, but
  an application must be ``aware'' of Chromium to take advantage of this
  parallel rendering.  We describe our efforts to make VTK applications
  Chromium aware.

  Another power wall display solution available to us is ICE-T, the Image
  Composite Engine for Tiles.  The ICE-T API is an implementation of the
  work presented by Moreland, Wylie, and Pavlakos for efficiently
  performing sort-last parallel rendering onto tile displays
  \cite{Moreland01}.  The ICE-T methodology for rendering fits well with
  VTK's existing parallel rendering paradigm.  We therefore implemented a
  new composite engine for VTK using the ICE-T API, to be discussed in
  section \ref{sec:tiled_display}.


  \section{Parallel Rendering Interface}
  \label{sec:parallel_rendering_interface}

  Parallel rendering in VTK is supported via the
  \cidentifier{vtk\-Composite\-Manager} class.  This class works by
  listening for render events on any renderable VTK window.  After each
  render, but before the images are presented to the user,
  \cidentifier{vtk\-Composite\-Manager} reads back the frame buffers,
  performs a sort-last compositing, and writes the image back to the frame
  buffer.

  While \cidentifier{vtk\-Composite\-Manager} provides much of the
  functionality we need for our parallel rendering tools, it is
  unnecessarily constrained to a single mode of parallel rendering.  Our
  initial approaches to creating parallel rendering modules involved
  subclassing \cidentifier{vtk\-Composite\-Manager}.  Unfortunately,
  \cidentifier{vtk\-Composite\-Manager} expects its subclasses to only
  handle image data that has already been read from framebuffers.
  Attempting to work around this limitation resulted in obfuscated code.

  Our response was to create a new class,
  \cidentifier{vtk\-Parallel\-Render\-Manager}.  It works in a manner
  similar to \cidentifier{vtk\-Composite\-Manager} in that they both listen
  to render events and perform appropriate actions around them.  The
  difference is \cidentifier{vtk\-Parallel\-Render\-Manager} is an abstract
  class designed to give subclasses as much or as little control as they
  need.

  \begin{figure}
    \begin{center}
      \includegraphics[width=\linewidth]
		      {images/ParallelRenderManagerInteraction}
    \end{center}
    \caption{Interactions of the parallel render manager during render.}
    \label{fig:parallel_render_manager_interaction}
  \end{figure}
  Figure \vref{fig:parallel_render_manager_interaction} outlines how the
  \cidentifier{vtk\-Parallel\-Render\-Manager} behaves during a render
  event.  After the ``master'' or ``root'' parallel render manager receives
  the start render event, it broadcasts an RMI to all other parallel render
  managers to also start a render.  It then broadcasts rendering
  information to all other parallel render managers.  The parallel render
  manager also calls protected virtual methods to allow subclasses to
  perform their own data synchronization.  Finally, the parallel render
  manager class calls another protected virtual function to allow a
  subclass to perform any other necessary processing before the actual
  render occurs.

  Control is relinquished back to the render window, which proceeds to
  perform the actual image synthesis.  Afterwards, each render window sends
  an end render event to its respective parallel render manager.  The
  \cidentifier{vtk\-Parallel\-Render\-Manager} calls yet another protected
  virtual function to allow subclasses to perform any post-processing, such
  as image composition.

  In addition to handling render events and their propogation and
  synchronization as described above,
  \cidentifier{vtk\-Parallel\-Render\-Manager} also has the following
  features.
  \begin{description}
    \item [Object Creation] Provides factory methods for creating renderer
      and render window objects.  This allows a parllel rendering scheme to
      perform operations in the middle of rendering by subclassing some of
      the objects that perform the actual render.
    \item [Boundries] Will compute the physical boundry, in three space, of
      the composite object rendered by all processes.
    \item [Image Reduction] Can reduce the size of the image being rendered
      and inflate the image for viewing.  Reducing the image size can
      drastically reduce the time required for image composition
      strategies.  The class can also automatically set the image reduction
      based on the user's desired rendering rate.
    \item [Image Caching] If the post render processing requires the image
      to be read from the graphics card's frame buffer, the image will be
      cached for potential \cidentifier{vtk\-Parallel\-Render\-Manager}
      users.
  \end{description}

  All of the parallel rendering classes described in this paper inherit
  from \cidentifier{vtk\-Parallel\-Render\-Manager}.  Doing this affords us
  a significant amount of code reuse.  It also provides an abstraction that
  alows us to easily swap the parallel rendering method in our
  applications.

  We have also implemented a class called
  \cidentifier{vtk\-Composite\-Render\-Manager}.  This class is identical
  in features to the \cidentifier{vtk\-Composite\-Manager} distributed with
  VTK.  However, having it also inherit from
  \cidentifier{vtk\-Parallel\-Render\-Manager} allows it to play with our
  applications along with our other parallel rendering classes.  The code
  for \cidentifier{vtk\-Composite\-Render\-Manager} is quite small since
  \cidentifier{vtk\-Parallel\-Render\-Manager} does most of the work.  It
  took very little time to code and debug
  \cidentifier{vtk\-Composite\-Render\-Manager}.  Our hope is to one day
  replace the composite manager in VTK with our own version.


  \section{Desktop Delivery}
  \label{sec:desktop_delivery}

  Since we in no way expect each analyst to have a parallel cluster
  available in his office, we find it important to provide the ability of
  remote desktop delivery.  That is, we wish a user to be able to
  interactively control a visualization job rendering on a cluster and
  display back to a typical desktop machine connected via a modest LAN
  network.

  To this end, we have built a pair of collaborating objects:
  \cidentifier{vtk\-Desktop\-Delivery\-Client} and
  \cidentifier{vtk\-Desktop\-Delivery\-Server}.  The client object is
  responsible for accepting user input and displaying rendered images.  The
  server object is responsible for the visualization processing and
  rendering.  The two objects are connected together with a pair of
  \cidentifier{vtk\-Multi\-Process\-Controller} objects.
  \cidentifier{vtk\-Multi\-Process\-Controller}, which is part of the VTK
  distribution, is an abstract interface to parallel process control and
  communication.  The desktop delivery objects expect the process
  controller to control exactly two objects with client and server in
  opposite processes.  The controller is typically implemented with a
  socket, but does not have to be.

  \begin{figure}[ht]
    \begin{center}
      \includegraphics[width=\linewidth]{images/DesktopDeliveryInteraction}
      \caption{Interactions during rendering with desktop delivery.}
      \label{fig:desktop_delivery_interaction}
    \end{center}
  \end{figure}

  The \cidentifier{vtk\-Desktop\-Delivery\-Client} object attaches itself
  as an observer to a render window.  When a render event occurs, the
  desktop delivery client object sends a render request to the server along
  with the rendering parameters, waits for an image to come back, and
  pastes the image to the window.  As one would expect, the
  \cidentifier{vtk\-Desktop\-Delivery\-Server} object responds to render
  requests from the client by invoking the render window and shipping the
  resulting image back.  Figure \vref{fig:desktop_delivery_interaction}
  shows the details of this process.

  The reader may notice a striking similarity between the operational
  features of the desktop delivery client/server and the parallel render
  manager described in section \ref{sec:parallel_rendering_interface}.  In
  particular, the interactions shown in figures
  \ref{fig:parallel_render_manager_interaction} and
  \ref{fig:desktop_delivery_interaction} are nearly identical.  Because of
  this, both \cidentifier{vtk\-Desktop\-Delivery\-Client} and
  \cidentifier{vtk\-Desktop\-Delivery\-Server} inherit from
  \cidentifier{vtk\-Parallel\-Render\-Manager}.  While this inheritance
  strains the ``is-a'' requirement dictated by good object oriented program
  design, we felt the amount of code re-use we achieved was too important
  to neglect.  Futhermore, by inheriting from
  \cidentifier{vtk\-Parallel\-Render\-Manager} we were able to take
  advantage of its boundry calculation and image reduction capabilities,
  both of which are vital properties of our desktop delivery.

  Because the desktop delivery server was designed to enable delivery from
  clusters, it can optionally work with another
  \cidentifier{vtk\-Parallel\-Render\-Manager} object.  The
  \cidentifier{vtk\-Desktop\-Delivery\-Server} performs image delivery
  after the other \cidentifier{vtk\-Parallel\-Render\-Manager} generates
  the image.  The desktop delivery server object uses the other parallel
  render manager to compute the complete object bounds, get timing
  statistics, and otherwise control the parallel rendering process.  The
  desktop delivery server will also take advantage of the other parallel
  render manager's image caching to avoid multiple reads and writes from
  the graphics card frame buffer.


  \section{Chromium}
  \label{sec:chromium}

  We also greatly desire to drive power walls in our parallel VTK
  applications.  One means of doing this is with the Chromium system
  \cite{Humphreys02}.  Chromium intercepts a stream of OpenGL commands and
  filters them.  Chromium's filters are pluggable components called stream
  processing units (SPUs).  One such unit distributed with Chromium is the
  \cidentifier{tile\-sort} SPU.  The \cidentifier{tile\-sort} SPU
  determines the area of the screen groups of primitives occupy and ships
  them to cluster nodes responsible for displaying that portion of the
  screen.

  Because Chromium has the ability to put itself in place of an OpenGL
  shared object library, it can provide a tile display and/or a number of
  other filtering operations to almost any OpenGL application without
  modification.  However, the OpenGL API is inherently serial, which can
  impose a huge bottleneck.  Chromium also supports parallel applications,
  but the application must be at least somewhat Chromium ``aware.''

  To make our VTK applications Chromium aware, we built the
  \cidentifier{vtk\-Cr\-Render\-Manager} object.  Because the parallel
  rendering is really being handled by Chromium,
  \cidentifier{vtk\-Cr\-Render\-Manager} does little but allow its
  superclass, \cidentifier{vtk\-Parallel\-Render\-Manager}, to propagate
  render events and parameters.  We also built
  \cidentifier{vtk\-Cr\-Open\-GL\-Render\-Window} and
  \cidentifier{vtk\-Cr\-Open\-GL\-Renderer} objects, which the Chromium
  render manager will build for the application.  They work very much like
  their OpenGL counterparts except that they can interface directly with
  the Chromium API.  Currently, they suppress the creation of unused
  windows, provide bounding box hints for \cidentifier{tile\-sort}, and
  provide some interaction with a few other select SPUs.  In time, the
  features of these objects may grow to strengthen the coupling between VTK
  applications and Chromium.

  Chromium scaling issues.


  \section{ICE-T}
  \label{sec:ICE-T}

  Another power wall display solution available to us is ICE-T, the Image
  Composite Engine for Tiles.  The ICE-T API is an implementation of the
  work presented by Moreland, Wylie, and Pavlakos for efficiently
  performing sort-last parallel rendering onto tile displays
  \cite{Moreland01}.


  \section{Tiled Display}
  \label{sec:tiled_display}

%%   \begin{figure*}
%%     \begin{center}
%%       \includegraphics[scale=0.23]{images/IceTCompositeClasses}
%%     \end{center}
%%     \caption{Tiled display compositing classes.}
%%     \label{fig:tiled_display_classes}
%%   \end{figure*}

  We also greatly desire to drive power walls in our parallel VTK
  applications.  We did this by wrapping our ICE-T API in VTK objects.
  First, we subclassed vtkCompositeManager to allow ICE-T to be a drop-in
  replacement for any VTK compositor.  Unfortunately, the ICE-T API
  exhibits behavior that goes above and beyond what the VTK composite
  manager does.  In particular, because ICE-T generates images larger than
  the renderable window of any given display, it must be able to render the
  same geometry multiple times.  It must also be able to change the
  projection matrix before each rendering.  This functionality was most
  easily introduced by subclassing vtkOpenGLRenderer.  Under ``normal''
  circumstances, the ICE-T renderer behaves just like any other VTK
  renderer.  When used in conjunction with the ICE-T composite manager, the
  ICE-T renderer invokes the ICE-T API to manipulate projection matrices,
  render multiple viewing frustums, and compose multiple images in
  parallel.  The ICE-T composite manager does nothing but emit errors if
  not used with an ICE-T renderer.  A diagram of how these objects
%%   collaborate is shown in figure \vref{fig:tiled_display_classes}.

  Like the desktop delivery and other forms of the vtkCompositeManager
  classes, the ICE-T composite manager also supports the idea of a
  reduction factor.  Unlike the other reduction factor implementations, the
  ICE-T composite manager does not reduce the size of the renderable
  window.  Recall that in a tiled display, the overall display size is
  larger than any one image.  ICE-T uses an extension of the ``floating
  viewport'' described in Moreland, Wylie, and Pavlakos \cite{Moreland01}
  to render images spanning multiple tiles and therefore reduce the number
  of times the geometry must be rendered.

  Another important issue with rendering to tiled displays is providing an
  interface with which users can interact.  The typical solution for VTK
  applications using a composite manager such as ParaView is to place the
  user interface at node 0 where the image is displayed \cite{Law01}.  This
  is problematic with a tile display for several reasons.  The most
  prevailing problem for us is the fact that, for security reasons, our
  display wall and driving cluster are in different rooms, separated by a
  bolted steel door.  Instead, our user interface resides on a ``steering
  station,'' a separate PC located in view of the display and connected to
  the cluster via a standard Ethernet connection.  Changes made in the user
  interface running on the steering station are then propagated to the
  images on the display wall.

  It so happens that this exact functionality is already implemented by the
  desktop delivery objects described in section \ref{sec:desktop_delivery}
  except that images are displayed on the server rather than shipped back
  the client.  Therefore, the desktop delivery server object has a flag to
  select the display.  If displaying to the client, the behavior is as
  described in section \ref{sec:desktop_delivery}.  If displaying to the
  server, the render windows on the server side are never resized and
  images are not transfered back to the client.  The client simply renders
  a very coarse representation of the geometry, which is a bounding box by
  default.


  \section{Discussion}
  \label{sec:discussion}

  With the introduction of the desktop delivery and ICE-T components into
  VTK, we have shown that VTK is a viable framework for cluster-based
  interactive applications that require remote display or display to
  high-resolution power walls.  By using both the image-centric
  level-of-detail provided by the reduction factors in conjunction with the
  geometry-centric level-of-detail directly provided by VTK, we can achieve
  highly interactive rendering with almost any image transfer, compositing,
  or rendering speeds.  \sticky{Some empirical evidence, i.e. with LLNL
  data, would probably be good here.  But that would suggest adding more
  figures, increasing the size of the paper, and making review and release
  harder.  Doing this probably depends on the forum.}

  One drawback of the ICE-T composite objects is that they only work with
  OpenGL rendering and MPI communications.  This is because, due to
  historical reasons, the ICE-T API is dependent on the OpenGL and MPI
  APIs.  In retrospect, coupling the compositing API with any particular
  rendering or communications API was not beneficial.  Nevertheless, OpenGL
  and MPI remain the \emph{de facto} standards.


  \section{Future Work}
  \label{sec:future_work}

  We have just begun to make a viable production-quality tool to run on our
  clusters and display on our power walls.  There is much work to be done.

  The use of ICE-T's image compositing to drive power walls has the
  advantage of being resilient to changes in the input geometry size.
  Unfortunately, it also incurs a high overhead in each frame regardless of
  how little geometry is to be rendered.  In situations like this, we could
  benefit greatly from a sort-first architecture like the one implemented
  in the ``tilesort'' SPU of the Chromium system \cite{Humphreys02}.  It
  would be interesting to directly compare the two approaches and find the
  cutoff in geometry size, if any, where image compositing outweighs tile
  sorting.

  Our desktop delivery objects allow for any type of interface to be built
  on the client side.  However, we currently have not built a GUI capable
  of much more than simple navigation controls.  Ultimately, we would like
  a fully featured visualization tool such as ParaView \cite{Law01} to be
  presented at the desktop.

  While the concept of a steering station (introduced in section
  \ref{sec:tiled_display}) allows an arbitrarily complex user interface, it
  can be less than ideal.  Often, we find the user jumping between the
  steering station and the wall.  We would like to incorporate a hand-held
  device that could move with the user and at least provide navigation
  controls.


  \section{Acknowledgments}

  This work was performed at Sandia National Laboratories.  Sandia is a
  multi-program laboratory operated by Sandia Corporation, a Lockheed
  Martin Company, for the United States Department of Energy under contract
  DE-AC04-94AL85000.


  \bibliographystyle{plain}
  \bibliography{vtkicet}

\end{document}
